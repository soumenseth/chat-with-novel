{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1fe8597-e6f5-461b-a582-9a189e0a5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_text_splitters.character import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import transformers \n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.llms import GPT4All\n",
    "# from gpt4all import GPT4All\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "import torch\n",
    "from typing import Union\n",
    "import pathlib\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b8d34-5093-4aa5-bcb5-1d6ea73a24bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1785e57-66f3-438a-b501-60c217f2a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        content = f.read().decode(\"utf-8\")\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3913e104-5565-4587-b250-2e1cf5f02dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = 'NVIDIA GeForce RTX 3050 Laptop GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c24c57-adb3-459f-9d65-177d85a2953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_name = \"The Wild Duck.txt\"\n",
    "# models_path = \"C:\\\\Users\\\\SOUMEN\\\\Projects\\\\Python\\\\chat-with\\\\models\"\n",
    "instruction_model_path = \"C:\\\\Users\\\\SOUMEN\\\\Projects\\\\Python\\\\chat-with-novel\\\\models\\\\tinyllama-1.1b-chat-v1.0.Q8_0.gguf\"\n",
    "# instruction_model_path = \"models\\\\tinyllama-1.1b-chat-v1.0.Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ad918-aa30-4813-a5b9-c1b4071d3bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d20745ec-3f15-414b-91c9-53b67c0e5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Novel:\n",
    "    \"\"\"\n",
    "    Python Class that reads a novel, creates a vector db of that novel and \n",
    "    provides an option to query through the novel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, novels_dir: Union[str, pathlib.Path], \n",
    "                 novel_name: str, \n",
    "                 instruction_model_path: Union[str, pathlib.Path],\n",
    "                 embedding_model_name: str=\"all-MiniLM-L6-v2.gguf2.f16.gguf\",\n",
    "                 device: str='NVIDIA GeForce RTX 3050 Laptop GPU'):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            novels_dir: Path to the directory which contains the novel.\n",
    "            novel_name: Name of the novel\n",
    "            embedding_model_name: Name of GPT4All or custom model that will be used for embedding. Including \".gguf\" file extension is optional but encouraged.\n",
    "            instruction_model_path: Path to the GPT4ALL or custom model that will be used for querying.\n",
    "            device: The processing unit on which the embedding model will run. \n",
    "                    It can be cpu, gpu or gpu device name. To know which gpu devices are available, call `GPT4All.list_gpus()` to check.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.novels_dir = novels_dir\n",
    "        self.name = novel_name\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.instruction_model_path = instruction_model_path\n",
    "        self.device = device\n",
    "        \n",
    "        self.instruction_model = None\n",
    "        \n",
    "        \n",
    "        self.novel_path = os.path.join(self.novels_dir, self.name)\n",
    "        \n",
    "        self.loader = TextLoader(self.novel_path)\n",
    "        self.documents = self.loader.load()\n",
    "        # self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "        #     # Set a really small chunk size, just to show.\n",
    "        #     chunk_size=1500,\n",
    "        #     chunk_overlap=200,\n",
    "        #     length_function=len,\n",
    "        #     is_separator_regex=False,\n",
    "        # )\n",
    "        self.text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "        \n",
    "        self.docs = self.text_splitter.split_documents(self.documents)\n",
    "        \n",
    "        self.novel_db = self.create_novel_db()\n",
    "    \n",
    "        self.template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(self.template)\n",
    "        self.output_parser = StrOutputParser()\n",
    "        \n",
    "    def create_novel_db(self):\n",
    "        \n",
    "        embeddings = GPT4AllEmbeddings(\n",
    "                model_name=self.embedding_model_name,\n",
    "                gpt4all_kwargs={'allow_download': 'True'}\n",
    "            )\n",
    "        novel_db = Chroma.from_documents(self.docs, embeddings)\n",
    "        \n",
    "        return novel_db\n",
    "    \n",
    "    def query(self, query):\n",
    "        \n",
    "        if self.instruction_model == None:\n",
    "            self.instruction_model = GPT4All(model=self.instruction_model_path, device=self.device)\n",
    "            \n",
    "        retriever = self.novel_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "        chain = RunnableMap({\n",
    "            \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        }) | self.prompt | self.instruction_model | self.output_parser\n",
    "        \n",
    "        result = chain.invoke({\"question\": query})\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5a84c95-3951-4c6d-9cd9-19a2cbd845e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the novel object\n",
    "novel = Novel(NOVELS_DIR, novel_name, instruction_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffa2ab-0b1e-4fed-8174-e90ef58aee68",
   "metadata": {},
   "source": [
    "#### Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1571b186-d5e9-44fe-aa8f-223f2c740eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SOUMEN\\anaconda3\\envs\\NovelTalk\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: The wild duck eats fish and other aquatic animals, such as frogs, turtles, and snakes.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novel.query(\"What does the duck eat?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noveltalk",
   "language": "python",
   "name": "noveltalk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
